{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chapter 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import brown\n",
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import gutenberg\n",
    "import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 1.Describe the class of strings matched by the following regular expressions.No code is needed and just describe what the following regular expressions do/match).\n",
    "\n",
    "#### a.[a-zA-Z]+\n",
    "#### b.[A-Z][a-z]*\n",
    "#### c.p[aeiou]{,2}t\n",
    "#### d.\\d+(\\.\\d+)?\n",
    "#### e.([^aeiou][aeiou][^aeiou])*\n",
    "#### f.\\w+(?:[-']\\w+)*|[-.(]+|\\S\\w*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a. Here we get an output of strings that contain one or more instances of a lowercase letter, uppercase letter or a mix of uppercase and lowercase letter.\n",
    "\n",
    "b. Here the output must have two characters with the first character being anything from uppercase A to uppercase Z. The second character can be anything from lowercase a to lowercase z. Because the * ends after the second character [a-z], that means we can have 0 or more instances of [a-z]\n",
    "\n",
    "c. The output that is returned are words that only have a max of 2 vowels [aeiou] between the letter p and t.\n",
    "\n",
    "d. The output can have one more instances of any decimal digit(0-9) followed by the scope of (\\.\\d+). Inside the scope, we need a .(decimal point) followeed by one ore more instances of any decimal digit (0-9). However, since ? follows the scope  (\\.\\d+), this mean that we have zero or one instances of it, since it is optional.\n",
    "\n",
    "e. Since the regular expressions are in (), they have a scope to follow. The start of the word can be any character other than a lowercase vowel. Meaning it can begin with uppercase vowels, non uppercase vowels or non-alphabetic characters. The next part of the expression contraints the second character to be a lowercase vowel. The last character is constrained to any character other than a lowercase vowel again. However, since the * follows the scope, this means that the output can have 0 or more instances of ([^aeiou][aeiou][^aeiou]).\n",
    "\n",
    "f. The output here can be several things since in the code it has | (the pipe character) which returns either whats on the left or right. So the first part of the expression is \\w+(?:[-']\\w+)*, which will return words that beginning with one more or instances of an alphanumeric character, which contain either a - or ' , which are followed by one ore more instances of an alphanumeric character. But because there is * after \\w+(?:[-']\\w+), it means there can be 0 or more instances of the word previously describe. \n",
    "\n",
    "Next, we look at the second part of the expression which is [-.(]+. This will return output that contains one or more instances of -, . or (, separately. So if the word has two - or two parethenes, that will be return separately. \n",
    "\n",
    "Lastly, the last part of the expression is \\S\\w*. The output returned will start with any non-white space character followed by zero or more instances of an alpha numeric character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.Rewrite the following loop as a list comprehension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "## result = []\n",
    "## for word in sent:\n",
    "##    word_len = (word, len(word))\n",
    "##    result.append(word_len)\n",
    "## result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('This', 4), ('is', 2), ('an', 2), ('introduction', 12), ('class', 5)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = ['This', 'is', 'an', 'introduction', 'class']\n",
    "\n",
    "result = [(w, len(w)) for w in sent]\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.Read in some text from your own document in your local disk, tokenize it, and use the regular expressions to  print the list of all wh-word types that occur. (wh-words in English are used in questions, relative clauses and exclamations: who, which, what, and so on.) (hint: import nltk;  import re; from nltk import word_tokenize) (use lower() and set())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"This is a cloudy, chilly and wet day.\\nFall weather is making it's approach very early, it seems.\\nHopefully, we can get one more weekend of warm weather before the fall settles in.\\nAfter, I need to study for exams and complete projects.\\nWhen I get tired or stressed out I like going to places where I can relax alone.\\nI am someone who enjoys their own quality time but I still enjoy hanging out with friends.\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "\n",
    "f = open(\"Oct_4_2021.txt\")\n",
    "raw = f.read()\n",
    "raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cloudy',\n",
       " ',',\n",
       " 'chilly',\n",
       " 'and',\n",
       " 'wet',\n",
       " 'day',\n",
       " '.',\n",
       " 'Fall',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'making',\n",
       " 'it',\n",
       " \"'s\",\n",
       " 'approach',\n",
       " 'very',\n",
       " 'early',\n",
       " ',',\n",
       " 'it',\n",
       " 'seems',\n",
       " '.',\n",
       " 'Hopefully',\n",
       " ',',\n",
       " 'we',\n",
       " 'can',\n",
       " 'get',\n",
       " 'one',\n",
       " 'more',\n",
       " 'weekend',\n",
       " 'of',\n",
       " 'warm',\n",
       " 'weather',\n",
       " 'before',\n",
       " 'the',\n",
       " 'fall',\n",
       " 'settles',\n",
       " 'in',\n",
       " '.',\n",
       " 'After',\n",
       " ',',\n",
       " 'I',\n",
       " 'need',\n",
       " 'to',\n",
       " 'study',\n",
       " 'for',\n",
       " 'exams',\n",
       " 'and',\n",
       " 'complete',\n",
       " 'projects',\n",
       " '.',\n",
       " 'When',\n",
       " 'I',\n",
       " 'get',\n",
       " 'tired',\n",
       " 'or',\n",
       " 'stressed',\n",
       " 'out',\n",
       " 'I',\n",
       " 'like',\n",
       " 'going',\n",
       " 'to',\n",
       " 'places',\n",
       " 'where',\n",
       " 'I',\n",
       " 'can',\n",
       " 'relax',\n",
       " 'alone',\n",
       " '.',\n",
       " 'I',\n",
       " 'am',\n",
       " 'someone',\n",
       " 'who',\n",
       " 'enjoys',\n",
       " 'their',\n",
       " 'own',\n",
       " 'quality',\n",
       " 'time',\n",
       " 'but',\n",
       " 'I',\n",
       " 'still',\n",
       " 'enjoy',\n",
       " 'hanging',\n",
       " 'out',\n",
       " 'with',\n",
       " 'friends',\n",
       " '.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'when', 'where', 'who'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(w.lower() for w in tokens if re.search('^(wh|Wh|WH|wH)',w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'when', 'where', 'who'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(w.lower() for w in tokens if re.findall(r'^.*(wh|Wh|WH|wH)',w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "## [w.lower() for w in tokens if w.lower().startswith('wh')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Create your own file consisting of words and (made up) frequencies, where each line consists of a word, the space character, and a positive integer, e.g. fuzzy 53. Read the file into a Python list using open(filename).readlines(). Next, break each line into its two fields using split(), and convert the number into an integer using int(). The result should be a list of the form: [['fuzzy', 53], ...]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = open(\"Random.txt\").readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = [l.split(' ') for l in content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['fuzzy', 53],\n",
       " ['hi', 12],\n",
       " ['want', 89],\n",
       " ['soccer', 2],\n",
       " ['baseball', 8],\n",
       " ['medicine', 89],\n",
       " ['football', 7],\n",
       " ['games', 10],\n",
       " ['sneakers', 8],\n",
       " ['basketball', 29],\n",
       " ['drinks', 21],\n",
       " ['space', 3],\n",
       " ['mars', 46],\n",
       " ['moon', 78],\n",
       " ['grass', 88],\n",
       " ['earth', 100],\n",
       " ['phone', 6],\n",
       " ['bed', 4],\n",
       " ['dogs', 77],\n",
       " ['cats', 36]]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line = [[num[0], int(num[1])] for num in line]\n",
    "line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5 .Readability measures are used to score the reading difficulty of a text, for the purposes of selecting texts of appropriate difficulty for language learners. Let us define μw to be the average number of letters per word, and μs to be the average number of words per sentence, in a given text. The Automated Readability Index (ARI) of the text is defined to be: 4.71 μw + 0.5 μs - 21.43. Compute the ARI score for each section of the Brown Corpus (i.e. News, Editorial,…, Humor). Please use two ways introduced in the class to calculate the average number of letters per word μw and the average number of words per sentences μs. You are supposed to get two different results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 15 adventure 24.40926324686098\n",
      "9 24 belles_lettres 31.095382353662615\n",
      "9 21 editorial 30.03411587902211\n",
      "8 16 fiction 25.150998441603406\n",
      "9 23 government 32.8804612028414\n",
      "9 20 hobbies 29.139922001742477\n",
      "8 21 humor 28.06731665647837\n",
      "9 24 learned 31.912251710365197\n",
      "9 23 lore 30.387050143555797\n",
      "8 15 mystery 24.159818927038003\n",
      "9 22 news 30.83395561360988\n",
      "9 23 religion 30.382863961972696\n",
      "9 23 reviews 31.13075310191683\n",
      "8 16 romance 24.713170386382934\n",
      "8 15 science_fiction 25.292578032827997\n"
     ]
    }
   ],
   "source": [
    "for category in brown.categories():\n",
    "    num_chars = len(brown.raw(categories = category))\n",
    "    num_words = len(brown.words(categories = category))\n",
    "    num_sents=len(brown.sents(categories = category))\n",
    "    ARI = (4.71*(num_chars/num_words)) + (0.5 *(num_words/num_sents)) - 21.43\n",
    "    print(round(num_chars/num_words), round(num_words/num_sents), category, ARI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 15 adventure 4.0841684990890705\n",
      "4 24 belles_lettres 10.987652885621749\n",
      "4 21 editorial 9.471025332953673\n",
      "4 16 fiction 4.9104735321302115\n",
      "5 23 government 12.08430349501021\n",
      "4 20 hobbies 8.922356393630267\n",
      "4 21 humor 7.887805248319808\n",
      "5 24 learned 11.926007043317348\n",
      "4 23 lore 10.254756197101155\n",
      "4 15 mystery 3.8335518942055167\n",
      "4 22 news 10.176684595052684\n",
      "4 23 religion 10.203109907301261\n",
      "4 23 reviews 10.769699888473433\n",
      "4 16 romance 4.34922419804213\n",
      "4 15 science_fiction 4.978058336905399\n"
     ]
    }
   ],
   "source": [
    "for category in brown.categories():\n",
    "    words = brown.words(categories = category)\n",
    "    sents = brown.sents(categories = category)\n",
    "    avg_word = sum(len(w) for w in words) / len(words)\n",
    "    avg_sent =  sum(len(s) for s in sents) / len(sents)\n",
    "    ARI = (4.71*avg_word) + (0.5 *avg_sent) - 21.43\n",
    "    print(round(avg_word), round(avg_sent), category, ARI)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.Use the Porter Stemmer to normalize some tokenized text (see below), calling the stemmer on each word. Do the same thing with the Lancaster Stemmer and describe any difference you observe by using these two stemmers. Finally, please try Lemmatization by using WordNet Lemmatizer and describe any difference you observe compared to Porter Stemmer and Lancaster Stemmer.\n",
    "\n",
    "#### text='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "text='Technologies based on NLP are becoming increasingly widespread. For example, phones and handheld computers support predictive text and handwriting recognition; web search engines give access to information locked up in unstructured text; machine translation allows us to retrieve texts written in Chinese and read them in Spanish; text analysis enables us to detect sentiment in tweets and blogs. By providing more natural human-machine interfaces, and more sophisticated access to stored information, language processing has come to play a central role in the multilingual information society'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technolog', 'base', 'on', 'nlp', 'are', 'becom', 'increasingli', 'widespread', '.', 'for', 'exampl', ',', 'phone', 'and', 'handheld', 'comput', 'support', 'predict', 'text', 'and', 'handwrit', 'recognit', ';', 'web', 'search', 'engin', 'give', 'access', 'to', 'inform', 'lock', 'up', 'in', 'unstructur', 'text', ';', 'machin', 'translat', 'allow', 'us', 'to', 'retriev', 'text', 'written', 'in', 'chines', 'and', 'read', 'them', 'in', 'spanish', ';', 'text', 'analysi', 'enabl', 'us', 'to', 'detect', 'sentiment', 'in', 'tweet', 'and', 'blog', '.', 'By', 'provid', 'more', 'natur', 'human-machin', 'interfac', ',', 'and', 'more', 'sophist', 'access', 'to', 'store', 'inform', ',', 'languag', 'process', 'ha', 'come', 'to', 'play', 'a', 'central', 'role', 'in', 'the', 'multilingu', 'inform', 'societi']\n"
     ]
    }
   ],
   "source": [
    "print([porter.stem(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['technolog', 'bas', 'on', 'nlp', 'ar', 'becom', 'increas', 'widespread', '.', 'for', 'exampl', ',', 'phon', 'and', 'handheld', 'comput', 'support', 'predict', 'text', 'and', 'handwrit', 'recognit', ';', 'web', 'search', 'engin', 'giv', 'access', 'to', 'inform', 'lock', 'up', 'in', 'unstruct', 'text', ';', 'machin', 'transl', 'allow', 'us', 'to', 'retriev', 'text', 'writ', 'in', 'chines', 'and', 'read', 'them', 'in', 'span', ';', 'text', 'analys', 'en', 'us', 'to', 'detect', 'senty', 'in', 'tweet', 'and', 'blog', '.', 'by', 'provid', 'mor', 'nat', 'human-machine', 'interfac', ',', 'and', 'mor', 'soph', 'access', 'to', 'stor', 'inform', ',', 'langu', 'process', 'has', 'com', 'to', 'play', 'a', 'cent', 'rol', 'in', 'the', 'multil', 'inform', 'socy']\n"
     ]
    }
   ],
   "source": [
    "print([lancaster.stem(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the porter and lancaster stemmers, we can see which that it performs stemming similarily. However, there are some words that are steemed differently such as in porter we have base, are, increasingli, phone, give, unstructur, etc. In lancaster we have some differences for those exact words such as bas, ar, increas, phon, unstruct. It seems stemming is done differently with each stemmer. Not sure what rules guide the porter and lancaster stemmer to perform their process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Technologies', 'based', 'on', 'NLP', 'are', 'becoming', 'increasingly', 'widespread', '.', 'For', 'example', ',', 'phone', 'and', 'handheld', 'computer', 'support', 'predictive', 'text', 'and', 'handwriting', 'recognition', ';', 'web', 'search', 'engine', 'give', 'access', 'to', 'information', 'locked', 'up', 'in', 'unstructured', 'text', ';', 'machine', 'translation', 'allows', 'u', 'to', 'retrieve', 'text', 'written', 'in', 'Chinese', 'and', 'read', 'them', 'in', 'Spanish', ';', 'text', 'analysis', 'enables', 'u', 'to', 'detect', 'sentiment', 'in', 'tweet', 'and', 'blog', '.', 'By', 'providing', 'more', 'natural', 'human-machine', 'interface', ',', 'and', 'more', 'sophisticated', 'access', 'to', 'stored', 'information', ',', 'language', 'processing', 'ha', 'come', 'to', 'play', 'a', 'central', 'role', 'in', 'the', 'multilingual', 'information', 'society']\n"
     ]
    }
   ],
   "source": [
    "wnl = nltk.WordNetLemmatizer()\n",
    "print([wnl.lemmatize(w) for w in tokens])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the WordNetLemmatizer, the differences are clearer when compared to porter and lancaster. First, the words don't change so they stay the same in terms of upper and lower case. The biggest difference I can see is that in the words us, tweets, blogs, and has, the WordNetLemmatizer removes the s, since I think it is reading those words as plural."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Please try to retreive some text from any web page in the form of HTML documents, tokenize the text, create an NLTK text and then use similar( ) function on any word of your interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.cnn.com/2021/10/04/tech/facebook-instagram-whatsapp-outage/index.html\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "tokens = word_tokenize(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Text: WhatsApp , Facebook , Instagram go down -...>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = nltk.Text(tokens)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 5 of 5 matches:\n",
      "WhatsApp , Facebook , Instagram go down - CNN MarketsTechMediaSuccessPerspec\n",
      "nseAmid firestorm , Facebook doubles down on researchLawmakers grill Facebook \n",
      "onvenience . `` Outage tracking site Down Detector logged tens of thousands of\n",
      " a massive part of the internet went down for an hourFirms sometimes lose inte\n",
      "e on Tuesday.Shares of Facebook were down more than 5 % in midday trading Mond\n"
     ]
    }
   ],
   "source": [
    "text.concordance('down')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 7 of 7 matches:\n",
      "pologize for any inconvenience . `` Outage tracking site Down Detector logged \n",
      "or send messages.The reason for the outage was not immediately clear . However\n",
      "Twitter that its tests indicate the outage is due to an ongoing DNS failure . \n",
      " MoreMore than four hours after the outage started , Facebook CTO Mark Schroep\n",
      ". `` I do n't know If I 've seen an outage like this before from a major inter\n",
      "which experienced a global internet outage for about 50 minutes.But the fact t\n",
      "d and propagated worldwide . `` The outage came the morning after `` 60 Minute\n"
     ]
    }
   ],
   "source": [
    "text.concordance('outage')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "the outage is due to an ongoing DNS failure . The DNS translates website names \n"
     ]
    }
   ],
   "source": [
    "text.concordance('failure')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "e aware that some people are having trouble accessing our apps and products , '\n"
     ]
    }
   ],
   "source": [
    "text.concordance('trouble')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.Rewrite the following nested loop by using a list comprehension and regular expressions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  words = ['attribution', 'confabulation', 'elocution',\n",
    "#         'sequoia', 'tenacious', 'unidirectional']\n",
    "#  vsequences = set()\n",
    "#  for word in words:\n",
    "#      vowels = []\n",
    "#      for char in word:\n",
    "#          if char in 'aeiou':\n",
    "#              vowels.append(char)\n",
    "#      vsequences.add(''.join(vowels))\n",
    "#  sorted(vsequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aiuio', 'eaiou', 'eouio', 'euoia', 'oauaio', 'uiieioa'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['attribution', 'confabulation', 'elocution', 'sequoia', 'tenacious', 'unidirectional']\n",
    "\n",
    "vsequences = set([''.join([char for char in word if re.findall(r'[aeiou]',char)]) for word in words])\n",
    "vsequences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.Try to refer the following sample code to print the following sentences in a formatted way.(Hint: you should use str.format() method in print() and for loop；For more information, please read the textbook section 3.9 in chapter 3) \n",
    "#### Output should exactly look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The Tragedie of Hamlet was written by William Shakespeare in 1599\n",
    "Leaves of Grass        was written by Walt Whiteman       in 1855\n",
    "Emma                   was written by Jane Austen         in 1816\n",
    "\n",
    "# sample code:\n",
    "template = 'Lee wants a {} right now'\n",
    "menu = ['sandwich', 'spam fritter', 'pancake']\n",
    "for snack in menu:\n",
    "    print(template.format(snack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Replacement index 1 out of range for positional args tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-33c76d703a9e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtemplate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m: Replacement index 1 out of range for positional args tuple"
     ]
    }
   ],
   "source": [
    "template = '{} was written by {} in {}'\n",
    "text = ['The tragedie of Hamlet', 'William Shakespeare', '1599', 'Leaves of Grass', 'Walt Whiteman', '1855', 'Emma', 'Jane Austen', '1816']\n",
    "\n",
    "for word in text:\n",
    "    print(template.format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I could not figure out the issue with my for loop from above but I know I have to use in order for the output to include all the text in on code. Below I printed out each sentence separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The tragedie of Hamlet was written by William Shakespeare in 1599'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{} was written by {} in {}'.format('The tragedie of Hamlet', 'William Shakespeare', 1599)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leaves of Grass was written by Walt Whiteman in 1855'"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{} was written by {} in {}'.format('Leaves of Grass', 'Walt Whiteman', '1855')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Emma was written by Jane Austen in 1816'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'{} was written by {} in {}'.format('Emma', 'Jane Austen', '1816')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10. Please first use sentence tokenization to print out the first 10 sentences in the \"carroll-alice.txt\" in the Gutenberg corpus. And then use basic corpus functionality sents() to return the first 10 sentences in this book. Please describe the difference between the two results.  (hint: use sent_tokenzie (), pprint(), sents())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['[', 'Alice', \"'\", 's', 'Adventures', 'in', 'Wonderland', 'by', 'Lewis', 'Carroll', '1865', ']'], ['CHAPTER', 'I', '.'], ['Down', 'the', 'Rabbit', '-', 'Hole'], ['Alice', 'was', 'beginning', 'to', 'get', 'very', 'tired', 'of', 'sitting', 'by', 'her', 'sister', 'on', 'the', 'bank', ',', 'and', 'of', 'having', 'nothing', 'to', 'do', ':', 'once', 'or', 'twice', 'she', 'had', 'peeped', 'into', 'the', 'book', 'her', 'sister', 'was', 'reading', ',', 'but', 'it', 'had', 'no', 'pictures', 'or', 'conversations', 'in', 'it', ',', \"'\", 'and', 'what', 'is', 'the', 'use', 'of', 'a', 'book', \",'\", 'thought', 'Alice', \"'\", 'without', 'pictures', 'or', 'conversation', \"?'\"], ['So', 'she', 'was', 'considering', 'in', 'her', 'own', 'mind', '(', 'as', 'well', 'as', 'she', 'could', ',', 'for', 'the', 'hot', 'day', 'made', 'her', 'feel', 'very', 'sleepy', 'and', 'stupid', '),', 'whether', 'the', 'pleasure', 'of', 'making', 'a', 'daisy', '-', 'chain', 'would', 'be', 'worth', 'the', 'trouble', 'of', 'getting', 'up', 'and', 'picking', 'the', 'daisies', ',', 'when', 'suddenly', 'a', 'White', 'Rabbit', 'with', 'pink', 'eyes', 'ran', 'close', 'by', 'her', '.'], ['There', 'was', 'nothing', 'so', 'VERY', 'remarkable', 'in', 'that', ';', 'nor', 'did', 'Alice', 'think', 'it', 'so', 'VERY', 'much', 'out', 'of', 'the', 'way', 'to', 'hear', 'the', 'Rabbit', 'say', 'to', 'itself', ',', \"'\", 'Oh', 'dear', '!'], ['Oh', 'dear', '!'], ['I', 'shall', 'be', 'late', \"!'\"], ['(', 'when', 'she', 'thought', 'it', 'over', 'afterwards', ',', 'it', 'occurred', 'to', 'her', 'that', 'she', 'ought', 'to', 'have', 'wondered', 'at', 'this', ',', 'but', 'at', 'the', 'time', 'it', 'all', 'seemed', 'quite', 'natural', ');', 'but', 'when', 'the', 'Rabbit', 'actually', 'TOOK', 'A', 'WATCH', 'OUT', 'OF', 'ITS', 'WAISTCOAT', '-', 'POCKET', ',', 'and', 'looked', 'at', 'it', ',', 'and', 'then', 'hurried', 'on', ',', 'Alice', 'started', 'to', 'her', 'feet', ',', 'for', 'it', 'flashed', 'across', 'her', 'mind', 'that', 'she', 'had', 'never', 'before', 'seen', 'a', 'rabbit', 'with', 'either', 'a', 'waistcoat', '-', 'pocket', ',', 'or', 'a', 'watch', 'to', 'take', 'out', 'of', 'it', ',', 'and', 'burning', 'with', 'curiosity', ',', 'she', 'ran', 'across', 'the', 'field', 'after', 'it', ',', 'and', 'fortunately', 'was', 'just', 'in', 'time', 'to', 'see', 'it', 'pop', 'down', 'a', 'large', 'rabbit', '-', 'hole', 'under', 'the', 'hedge', '.'], ['In', 'another', 'moment', 'down', 'went', 'Alice', 'after', 'it', ',', 'never', 'once', 'considering', 'how', 'in', 'the', 'world', 'she', 'was', 'to', 'get', 'out', 'again', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "\n",
    "carroll_sents = gutenberg.sents('carroll-alice.txt')\n",
    "print(carroll_sents[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\\n\\nCHAPTER I.\",\n",
      " 'Down the Rabbit-Hole\\n'\n",
      " '\\n'\n",
      " 'Alice was beginning to get very tired of sitting by her sister on the\\n'\n",
      " 'bank, and of having nothing to do: once or twice she had peeped into the\\n'\n",
      " 'book her sister was reading, but it had no pictures or conversations in\\n'\n",
      " \"it, 'and what is the use of a book,' thought Alice 'without pictures or\\n\"\n",
      " \"conversation?'\",\n",
      " 'So she was considering in her own mind (as well as she could, for the\\n'\n",
      " 'hot day made her feel very sleepy and stupid), whether the pleasure\\n'\n",
      " 'of making a daisy-chain would be worth the trouble of getting up and\\n'\n",
      " 'picking the daisies, when suddenly a White Rabbit with pink eyes ran\\n'\n",
      " 'close by her.',\n",
      " 'There was nothing so VERY remarkable in that; nor did Alice think it so\\n'\n",
      " \"VERY much out of the way to hear the Rabbit say to itself, 'Oh dear!\",\n",
      " 'Oh dear!',\n",
      " \"I shall be late!'\",\n",
      " '(when she thought it over afterwards, it\\n'\n",
      " 'occurred to her that she ought to have wondered at this, but at the time\\n'\n",
      " 'it all seemed quite natural); but when the Rabbit actually TOOK A WATCH\\n'\n",
      " 'OUT OF ITS WAISTCOAT-POCKET, and looked at it, and then hurried on,\\n'\n",
      " 'Alice started to her feet, for it flashed across her mind that she had\\n'\n",
      " 'never before seen a rabbit with either a waistcoat-pocket, or a watch\\n'\n",
      " 'to take out of it, and burning with curiosity, she ran across the field\\n'\n",
      " 'after it, and fortunately was just in time to see it pop down a large\\n'\n",
      " 'rabbit-hole under the hedge.',\n",
      " 'In another moment down went Alice after it, never once considering how\\n'\n",
      " 'in the world she was to get out again.',\n",
      " 'The rabbit-hole went straight on like a tunnel for some way, and then\\n'\n",
      " 'dipped suddenly down, so suddenly that Alice had not a moment to think\\n'\n",
      " 'about stopping herself before she found herself falling down a very deep\\n'\n",
      " 'well.',\n",
      " 'Either the well was very deep, or she fell very slowly, for she had\\n'\n",
      " 'plenty of time as she went down to look about her and to wonder what was\\n'\n",
      " 'going to happen next.']\n"
     ]
    }
   ],
   "source": [
    "text = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n",
    "sents = nltk.sent_tokenize(text)\n",
    "pprint.pprint(sents[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, when use sent_tokenzie () and pprint() we get alot more text compared to when we only use sents(). The first example is short because it is not split up as much. The lists within the list contain all the punctations and do not split much up as much. But in the second example, when we use sent_tokenzie and pprint, we see that some sentences have been broken up into different strings, which is why more text is included in the second output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
